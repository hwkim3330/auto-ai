#!/usr/bin/env python3
"""
LiquidAI LFM2 Model Runner
--------------------------
LiquidAIì˜ LFM2-1.2B ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
Liquid Foundation Models - íš¨ìœ¨ì ì¸ ì˜¨ë””ë°”ì´ìŠ¤ AI
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import time

def main():
    print("=" * 60)
    print("LiquidAI LFM2-1.2B Model Test")
    print("=" * 60)

    # Model selection - LFM2-1.2B is good balance of speed/quality
    model_id = "LiquidAI/LFM2-1.2B"

    print(f"\nğŸ“¥ Loading model: {model_id}")
    print("   (First run will download ~2.4GB)")

    start_time = time.time()

    # Load tokenizer
    print("\nğŸ”¤ Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)

    # Load model - force CPU (GTX 1050 Ti CUDA 6.1 not supported by PyTorch)
    print("ğŸ§  Loading model...")
    device = "cpu"  # Force CPU - older GPU not supported
    print(f"   Using device: {device}")

    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float32,
        device_map=None,
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    model = model.to(device)

    load_time = time.time() - start_time
    print(f"âœ… Model loaded in {load_time:.2f}s")

    # Test prompts
    prompts = [
        "What is Time-Sensitive Networking (TSN)?",
        "Write a Python function to calculate factorial:",
        "ì˜¤ëŠ˜ ì„œìš¸ì˜ ë‚ ì”¨ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”."  # Korean test
    ]

    print("\n" + "=" * 60)
    print("ğŸ§ª Running inference tests")
    print("=" * 60)

    for i, prompt in enumerate(prompts, 1):
        print(f"\n--- Test {i} ---")
        print(f"ğŸ“ Prompt: {prompt}")

        start_time = time.time()

        inputs = tokenizer(prompt, return_tensors="pt").to(device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=tokenizer.eos_token_id
            )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        gen_time = time.time() - start_time

        print(f"ğŸ¤– Response:\n{response}")
        print(f"â±ï¸  Generation time: {gen_time:.2f}s")

    print("\n" + "=" * 60)
    print("âœ… LiquidAI LFM2-1.2B Test Complete!")
    print("=" * 60)

    # Model info
    param_count = sum(p.numel() for p in model.parameters())
    print(f"\nğŸ“Š Model Statistics:")
    print(f"   Parameters: {param_count / 1e9:.2f}B")
    print(f"   Device: {device}")
    print(f"   Dtype: {next(model.parameters()).dtype}")

if __name__ == "__main__":
    main()
